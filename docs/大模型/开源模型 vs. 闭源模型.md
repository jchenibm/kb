# 开源模型 vs. 闭源模型

#### 概要

开源模型和闭源模型都有各自的优缺点，但是高质量的对齐是提高开源模型性能的关键。LLaMA-2论文发现，**对齐质量是开源和专有语言学习模型（LLMs）性能差距的主要原因**。闭源产品LLMs的高对齐度是其表现更出色的关键，但需要大量的计算和人工标注成本，并且通常不透明或难以复制。为了避免这些成本，大多数开源模型都是通过在公开可用的数据集上进行SFT进行对齐。然而，这种方法存在质量和多样性不足的问题，需要将研究重点放在高质量的对齐上。

#### 亮点

- ⚠️ 对齐质量是开源和专有语言学习模型性能差距的主要原因。
- 📈 闭源产品LLMs的高对齐度是其表现更出色的关键。
- 💻 大多数开源模型都是通过在公开可用的数据集上进行SFT进行对齐，存在质量和多样性不足的问题。
- 💡 高质量的对齐是提高开源模型性能的关键。
- 📊 LLaMA-2的性能远超现有的开源LLMs，包括MPT-30B和Falcon-40B等顶级模型。
- 💬 LLaMA-2-Chat甚至可以与ChatGPT等模型相提并论。
- 📚 需要更多的研究和投入来提高开源模型的性能。
